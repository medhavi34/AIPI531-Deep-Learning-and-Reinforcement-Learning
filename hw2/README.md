# DQN and Double DQN with Stable-Baselines3

This repository showcases the implementation and comparison of DQN (Deep Q-Network) and Double DQN using Stable Baselines3, a popular framework for training reinforcement learning agents.

## Introduction
This project focuses on the study and implementation of DQN using Stable-Baselines3 and demonstrates how to mitigate value overestimation using Double DQN. The experiments are conducted within the Mountain Car problem context, where the agent must navigate an underpowered car up a steep mountain road.

## Installation
The project uses the master version of Stable Baselines3. Detailed instructions for setting up the environment and dependencies are provided in the notebooks.

## Key Features
- **DQN and Double DQN**: Implementation of both DQN and its variant, Double DQN, to highlight the differences and improvements.
- **Value Overestimation Mitigation**: Demonstrates how Double DQN reduces the overestimation of values that is common in standard DQN.
- **Comparison and Analysis**: Comprehensive comparison between DQN and Double DQN in terms of performance and efficiency.

## Usage
1. **Setup**: Follow the installation instructions to set up Stable Baselines3 and other dependencies.
2. **Running the Experiments**: Step-by-step guide on running the DQN and Double DQN implementations in the Mountain Car environment.
3. **Analysis**: In-depth analysis of the results, highlighting the effectiveness of Double DQN in comparison to standard DQN.

## Repository Contents
- Jupyter Notebooks: Detailed code and explanations for implementing and comparing DQN and Double DQN.
- Additional Resources: Links to relevant documentation and research papers.

## Getting Started
To replicate the experiments and understand the methodologies used, follow the instructions and code provided in the `dqn_sb3.ipynb` notebook.
